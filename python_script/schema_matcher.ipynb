{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valentine import valentine_match, valentine_metrics\n",
    "from valentine.algorithms import Coma\n",
    "from valentine.algorithms import Cupid\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pandas as pd\n",
    "from compute_match import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparazione data frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dizionari che associa il nome del file al file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset sono in formato json necessariamente\n",
    "path = \"../json_datasets\"\n",
    "\n",
    "# Dizionario Key=Nome del file, Value=File json associato (letto da filepath)\n",
    "data_frames = {}\n",
    "# Per tutti i file dentro la cartella dei dataset\n",
    "for file in os.listdir(path):\n",
    "    # Prendo il path del file\n",
    "    filepath = f\"{path}/{file}\"\n",
    "    # Update del dizionario\n",
    "    data_frames[file] = (pd.read_json(filepath))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione di tutte le coppie di dataset da valutare (evitando ripetizioni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendo tutti i nomi dei file\n",
    "keys = list(data_frames.keys())\n",
    "tuples = []\n",
    "# Il numero di dataframes\n",
    "dfs_len = len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preso un dataframe, lo confronto con tutti i dataframes successivi (così evito di confrontare coppie già confrontate)\n",
    "for i in range(dfs_len):\n",
    "    for j in range(i + 1, dfs_len):\n",
    "        dfl = data_frames[keys[i]]\n",
    "        dfr = data_frames[keys[j]]\n",
    "        # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "        tuples.append((keys[i], keys[j], dfl, dfr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valentine schema matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_multithread(tuples):\n",
    "    result = []\n",
    "    # Creo un pool per l'esecuzione multi processore (per velocizzare il matching tra tutte le coppie di dataframes)\n",
    "    with Pool() as pool:\n",
    "        # Ogni pool esegue la funzione (primo parametro), passandogli le tuple (secondo parametro)\n",
    "        # result è una lista di tuple ove una tupla è il risultato della funzione mappata\n",
    "        result = pool.map(calculate_match_coma_schema, tuples)\n",
    "    return result\n",
    "\n",
    "def matches_to_table(result):\n",
    "    matches = pd.DataFrame()\n",
    "    # Per ogni risultato ottenuto dal matching\n",
    "    for match in result:\n",
    "    # Prendo il valore ritornato dalla funzione valentine_match \n",
    "    # e.g. match[2] = ((table_1, 'Cited by'),(table_2, 'Cited by')): 0.83\n",
    "        for key in match[2].keys():\n",
    "            # e.g. key = (table_1, 'Cited by'),(table_2, 'Cited by')\n",
    "            d = dict()\n",
    "            d[\"table_1\"] = match[0]\n",
    "            d[\"table_2\"] = match[1]\n",
    "            d[\"column_table_1\"] = key[0][1]\n",
    "            d[\"column_table_2\"] = key[1][1]\n",
    "            d[\"match_value\"] = match[2][key]\n",
    "            # Aggiungiamo al dataframe l'ennupla\n",
    "            matches = pd.concat([matches, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    return matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabella per la visualizzazione dei risultati dello schema matching con Valentine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Silvestri-value.today.json', 'Gren-disfold.com.json', {}), ('Silvestri-value.today.json', 'Wissel-ariregister.rik.ee.json', {}), ('Silvestri-value.today.json', 'MalPatSaj-disfold.com.json', {}), ('Silvestri-value.today.json', 'DDD-ft.com.json', {}), ('Gren-disfold.com.json', 'Wissel-ariregister.rik.ee.json', {}), ('Gren-disfold.com.json', 'MalPatSaj-disfold.com.json', {(('table_1', 'industry'), ('table_2', 'Industry')): 0.9352676, (('table_1', 'country'), ('table_2', 'Country')): 0.9333069, (('table_1', 'sector'), ('table_2', 'Sector')): 0.931101, (('table_1', 'stock'), ('table_2', 'Stock')): 0.9279007, (('table_1', 'name'), ('table_2', 'Name')): 0.9257439, (('table_1', 'market_capitalization_USD'), ('table_2', 'MarketCap')): 0.4048583}), ('Gren-disfold.com.json', 'DDD-ft.com.json', {}), ('Wissel-ariregister.rik.ee.json', 'MalPatSaj-disfold.com.json', {}), ('Wissel-ariregister.rik.ee.json', 'DDD-ft.com.json', {}), ('MalPatSaj-disfold.com.json', 'DDD-ft.com.json', {})]\n",
      "                 table_1                     table_2  \\\n",
      "0  Gren-disfold.com.json  MalPatSaj-disfold.com.json   \n",
      "1  Gren-disfold.com.json  MalPatSaj-disfold.com.json   \n",
      "2  Gren-disfold.com.json  MalPatSaj-disfold.com.json   \n",
      "3  Gren-disfold.com.json  MalPatSaj-disfold.com.json   \n",
      "4  Gren-disfold.com.json  MalPatSaj-disfold.com.json   \n",
      "5  Gren-disfold.com.json  MalPatSaj-disfold.com.json   \n",
      "\n",
      "              column_table_1 column_table_2  match_value  \n",
      "0                   industry       Industry     0.935268  \n",
      "1                    country        Country     0.933307  \n",
      "2                     sector         Sector     0.931101  \n",
      "3                      stock          Stock     0.927901  \n",
      "4                       name           Name     0.925744  \n",
      "5  market_capitalization_USD      MarketCap     0.404858  \n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "result = calculate_match_multithread(tuples)\n",
    "finish_time = time.perf_counter()\n",
    "\n",
    "# Definizione del nome delle colonne della tabella\n",
    "columns = [\"table_1\", \"table_2\", \"column_table_1\", \"column_table_2\", \"match_value\"]\n",
    "# Creo il dataframe (la tabella in questione)\n",
    "matches = pd.DataFrame(columns=columns)\n",
    "\n",
    "matches = matches_to_table(result)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(matches)\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39m./instance_matches.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matches' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(matches).to_csv(\"./instance_matches.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema mediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediated_schema_columns = ('name', 'country', 'market cap', 'founded year', 'employees', 'industry', 'sector',\n",
    "     'ceo', 'revenue', 'Stock', 'share price', 'city', 'address', 'website')\n",
    "\n",
    "mediated_schema = pd.DataFrame(columns=mediated_schema_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_len = len(keys)\n",
    "matches_with_mediated = {}\n",
    "for i in range(dfs_len):\n",
    "    dfl = mediated_schema\n",
    "    dfr = data_frames[keys[i]]\n",
    "    # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "    tuples.append(('mediated_schema', keys[i], dfl, dfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = calculate_match_multithread(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matches \u001b[39m=\u001b[39m matches_to_table(result)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(matches)\n",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mmatches_to_table\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m     23\u001b[0m         d[\u001b[39m\"\u001b[39m\u001b[39mmatch_value\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m match[\u001b[39m2\u001b[39m][key]\n\u001b[1;32m     24\u001b[0m         \u001b[39m# Aggiungiamo al dataframe l'ennupla\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m         matches \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([result, pd\u001b[39m.\u001b[39;49mDataFrame(d, index\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m])], ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m matches\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/IDD/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/IDD/lib/python3.10/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/IDD/lib/python3.10/site-packages/pandas/core/reshape/concat.py:458\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[1;32m    454\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    455\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot concatenate object of type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39monly Series and DataFrame objs are valid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m         )\n\u001b[0;32m--> 458\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    460\u001b[0m     ndims\u001b[39m.\u001b[39madd(obj\u001b[39m.\u001b[39mndim)\n\u001b[1;32m    462\u001b[0m \u001b[39m# get the sample\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39m# want the highest ndim that we have, and must be non-empty\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m# unless all objs are empty\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "matches = matches_to_table(result)\n",
    "print(matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ce6619c7d95fa66adab3aa560eb20f28a01e1f23fb927f9fbc92f5fa3f739e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
