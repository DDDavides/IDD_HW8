{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valentine import valentine_match, valentine_metrics\n",
    "from valentine.algorithms import Coma\n",
    "from valentine.algorithms import Cupid\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pandas as pd\n",
    "from compute_match import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparazione data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './csv/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dizionari che associa il nome del file al file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset sono in formato json necessariamente\n",
    "path = \"../../json_datasets\"\n",
    "\n",
    "# Dizionario Key=Nome del file, Value=File json associato (letto da filepath)\n",
    "data_frames = {}\n",
    "# Per tutti i file dentro la cartella dei dataset\n",
    "for file in os.listdir(path):\n",
    "    # Prendo il path del file\n",
    "    filepath = f\"{path}/{file}\"\n",
    "    # Update del dizionario\n",
    "    data_frames[file] = (pd.read_json(filepath))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione di tutte le coppie di dataset da valutare (evitando ripetizioni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendo tutti i nomi dei file\n",
    "dbFileNames = list(data_frames.keys())\n",
    "# Tuple = (nome dataset1, nome dataset2, dataframe1, dataframe2)\n",
    "allDbsCouples = []\n",
    "# Il numero di dataframes\n",
    "dfs_len = len(dbFileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preso un dataframe, lo confronto con tutti i dataframes successivi (così evito di confrontare coppie già confrontate)\n",
    "for i in range(dfs_len):\n",
    "    for j in range(i + 1, dfs_len):\n",
    "        dfl = data_frames[dbFileNames[i]]\n",
    "        dfr = data_frames[dbFileNames[j]]\n",
    "        # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "        allDbsCouples.append((dbFileNames[i], dbFileNames[j], dfl, dfr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valentine schema matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_multithread(tuples):\n",
    "    result = []\n",
    "    # Creo un pool per l'esecuzione multi processore (per velocizzare il matching tra tutte le coppie di dataframes)\n",
    "    with Pool() as pool:\n",
    "        # Ogni pool esegue la funzione (primo parametro), passandogli le tuple (secondo parametro)\n",
    "        # result è una lista di tuple ove una tupla è il risultato della funzione mappata\n",
    "        result = pool.map(calculate_match_coma_schema, tuples)\n",
    "    return result\n",
    "\n",
    "def matches_to_table(result):\n",
    "    matches = pd.DataFrame()\n",
    "    # Per ogni risultato ottenuto dal matching\n",
    "    for match in result:\n",
    "    # Prendo il valore ritornato dalla funzione valentine_match \n",
    "    # e.g. match[2] = ((table_1, 'Cited by'),(table_2, 'Cited by')): 0.83\n",
    "        for key in match[2].keys():\n",
    "            # e.g. key = (table_1, 'Cited by'),(table_2, 'Cited by')\n",
    "            d = dict()\n",
    "            d[\"table_1\"] = match[0]\n",
    "            d[\"table_2\"] = match[1]\n",
    "            d[\"column_table_1\"] = key[0][1]\n",
    "            d[\"column_table_2\"] = key[1][1]\n",
    "            d[\"match_value\"] = match[2][key]\n",
    "            # Aggiungiamo al dataframe l'ennupla\n",
    "            matches = pd.concat([matches, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    return matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabella per la visualizzazione dei risultati dello schema matching con Valentine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.perf_counter()\n",
    "# # Calcola i match per tutte le coppie di datasets definite in tuples\n",
    "# result = calculate_match_multithread(allDbsCouples)\n",
    "# finish_time = time.perf_counter()\n",
    "\n",
    "# # Definizione del nome delle colonne della tabella\n",
    "# columns = [\"table_1\", \"table_2\", \"column_table_1\", \"column_table_2\", \"match_value\"]\n",
    "# # Creo il dataframe (la tabella in questione)\n",
    "# matches = pd.DataFrame(columns=columns)\n",
    "\n",
    "# # Porta tutti i match in formato tabellare\n",
    "# matches = matches_to_table(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(matches).to_csv(csv_path + \"matches.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema mediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema mediato definito manualmente tramite attributi di interesse e che fanno maggior match\n",
    "mediated_schema_columns = ['name', 'country', 'market cap', 'founded year', 'employees', 'industry', 'sector',\n",
    "     'ceo', 'revenue', 'stock', 'share price', 'city', 'address', 'website']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediated_schema = pd.DataFrame(columns=mediated_schema_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione di tutte le coppie (schema-mediato, dataset)\n",
    "matches_with_mediated = {}\n",
    "all_mediatedNDbs_couples = []\n",
    "for i in range(dfs_len):\n",
    "    dfl = mediated_schema\n",
    "    dfr = data_frames[dbFileNames[i]]\n",
    "    # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "    all_mediatedNDbs_couples.append(('mediated_schema', dbFileNames[i], dfl, dfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches con lo schema mediato\n",
    "result = calculate_match_multithread(all_mediatedNDbs_couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches_to_table(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matches).to_csv(csv_path + \"matches_with_mediated.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riempimento schema mediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv(csv_path + 'matches_with_mediated.csv')\n",
    "tables_to_columns = dict()\n",
    "# Per tutte le tabelle in match nella colonna table 2 vado a prendermi\n",
    "# le colonne con match >= 0.5 con una colonna dello schema mediato\n",
    "for table_name in set(matches['table_2']):\n",
    "    # Prende le colonne con table 2 uguale a table name\n",
    "    table = matches.loc[matches['table_2'] == table_name]\n",
    "    columns = set()\n",
    "    for ind in table.index:\n",
    "        if table['match_value'][ind] >= 0.5:\n",
    "            columns.add((table['column_table_1'][ind], table['column_table_2'][ind]))\n",
    "    tables_to_columns[table_name] = columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risoluzione problemi con colonne duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "schema_final = pd.DataFrame(columns=mediated_schema_columns)\n",
    "\n",
    "# Per tutti i dataset vai a inserire i dati nello schema mediato\n",
    "for df_name in tables_to_columns.keys():\n",
    "    df_aligned = pd.DataFrame()\n",
    "    key2maxYear = {}\n",
    "    colsToRemove = []\n",
    "\n",
    "    for match in tables_to_columns[df_name]:\n",
    "\n",
    "        if match[0] not in key2maxYear:\n",
    "            if re.findall(\"[0-9]{4}\", match[1]) != []:\n",
    "                key2maxYear[match[0]] = re.findall(\"[0-9]{4}\", match[1])[0], match[1]\n",
    "            else:\n",
    "                key2maxYear[match[0]] = 0, match[1]\n",
    "        else:\n",
    "            currYear = re.findall(\"[0-9]{4}\", match[1])[0]\n",
    "\n",
    "            if key2maxYear[match[0]][0] < currYear:\n",
    "                key2maxYear[match[0]] = currYear, match[1]\n",
    "                \n",
    "    # Per tutti i match del dataset effettua una proiezione sulle colonne in match e rinominale per allinearle allo schema mediato\n",
    "    for mediated_column in key2maxYear:\n",
    "        tmp = data_frames[df_name][key2maxYear[mediated_column][1]].to_frame()\n",
    "        tmp = tmp.rename(columns={key2maxYear[mediated_column][1]: mediated_column})\n",
    "\n",
    "        df_aligned = pd.concat([df_aligned, tmp], axis=1)\n",
    "    \n",
    "    try:\n",
    "        schema_final = pd.concat([schema_final, df_aligned], ignore_index=True, sort=False)\n",
    "    except:\n",
    "        print(\"Cannot concat schema final with: \", df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_final.to_csv(csv_path + \"schema_final.csv\", index_label='id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset con prime 10 ennuple (usate per magellan su colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sch = schema_final.iloc[:10]\n",
    "# sch.to_csv(csv_path + \"schema_final_10.csv\", index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75ba0f077842d729852f72f653d8bd29b9d81a568bccb91dac0b85029caf3bfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
