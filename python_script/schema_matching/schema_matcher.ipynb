{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valentine import valentine_match, valentine_metrics\n",
    "from valentine.algorithms import Coma\n",
    "from valentine.algorithms import Cupid\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pandas as pd\n",
    "from compute_match import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparazione data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './csv/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dizionari che associa il nome del file al file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset sono in formato json necessariamente\n",
    "path = \"../../json_datasets\"\n",
    "\n",
    "# Dizionario Key=Nome del file, Value=File json associato (letto da filepath)\n",
    "data_frames = {}\n",
    "# Per tutti i file dentro la cartella dei dataset\n",
    "for file in os.listdir(path):\n",
    "    # Prendo il path del file\n",
    "    filepath = f\"{path}/{file}\"\n",
    "    # Update del dizionario\n",
    "    data_frames[file] = (pd.read_json(filepath))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione di tutte le coppie di dataset da valutare (evitando ripetizioni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendo tutti i nomi dei file\n",
    "keys = list(data_frames.keys())\n",
    "tuples = []\n",
    "# Il numero di dataframes\n",
    "dfs_len = len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preso un dataframe, lo confronto con tutti i dataframes successivi (così evito di confrontare coppie già confrontate)\n",
    "for i in range(dfs_len):\n",
    "    for j in range(i + 1, dfs_len):\n",
    "        dfl = data_frames[keys[i]]\n",
    "        dfr = data_frames[keys[j]]\n",
    "        # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "        tuples.append((keys[i], keys[j], dfl, dfr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valentine schema matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_multithread(tuples):\n",
    "    result = []\n",
    "    # Creo un pool per l'esecuzione multi processore (per velocizzare il matching tra tutte le coppie di dataframes)\n",
    "    with Pool() as pool:\n",
    "        # Ogni pool esegue la funzione (primo parametro), passandogli le tuple (secondo parametro)\n",
    "        # result è una lista di tuple ove una tupla è il risultato della funzione mappata\n",
    "        result = pool.map(calculate_match_coma_schema, tuples)\n",
    "    return result\n",
    "\n",
    "def matches_to_table(result):\n",
    "    matches = pd.DataFrame()\n",
    "    # Per ogni risultato ottenuto dal matching\n",
    "    for match in result:\n",
    "    # Prendo il valore ritornato dalla funzione valentine_match \n",
    "    # e.g. match[2] = ((table_1, 'Cited by'),(table_2, 'Cited by')): 0.83\n",
    "        for key in match[2].keys():\n",
    "            # e.g. key = (table_1, 'Cited by'),(table_2, 'Cited by')\n",
    "            d = dict()\n",
    "            d[\"table_1\"] = match[0]\n",
    "            d[\"table_2\"] = match[1]\n",
    "            d[\"column_table_1\"] = key[0][1]\n",
    "            d[\"column_table_2\"] = key[1][1]\n",
    "            d[\"match_value\"] = match[2][key]\n",
    "            # Aggiungiamo al dataframe l'ennupla\n",
    "            matches = pd.concat([matches, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    return matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabella per la visualizzazione dei risultati dello schema matching con Valentine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "# Calcola i match per tutte le coppie di datasets definite in tuples\n",
    "result = calculate_match_multithread(tuples)\n",
    "finish_time = time.perf_counter()\n",
    "\n",
    "# Definizione del nome delle colonne della tabella\n",
    "columns = [\"table_1\", \"table_2\", \"column_table_1\", \"column_table_2\", \"match_value\"]\n",
    "# Creo il dataframe (la tabella in questione)\n",
    "matches = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Porta tutti i match in formato tabellare\n",
    "matches = matches_to_table(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matches).to_csv(csv_path + \"matches.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema mediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema mediato definito manualmente tramite attributi di interesse e che fanno maggior match\n",
    "mediated_schema_columns = ('name', 'country', 'market cap', 'founded year', 'employees', 'industry', 'sector',\n",
    "     'ceo', 'revenue', 'Stock', 'share price', 'city', 'address', 'website')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mediated_schema = pd.DataFrame(columns=mediated_schema_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione di tutte le coppie (schema-mediato, dataset)\n",
    "matches_with_mediated = {}\n",
    "for i in range(dfs_len):\n",
    "    dfl = mediated_schema\n",
    "    dfr = data_frames[keys[i]]\n",
    "    # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "    tuples.append(('mediated_schema', keys[i], dfl, dfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches con lo schema mediato\n",
    "result = calculate_match_multithread(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matches_to_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matches \u001b[39m=\u001b[39m matches_to_table(result)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(matches[\u001b[39m'\u001b[39m\u001b[39mtable_2\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matches_to_table' is not defined"
     ]
    }
   ],
   "source": [
    "matches = matches_to_table(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matches).to_csv(csv_path + \"matches_with_mediated.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riempimento schema mediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv(csv_path + 'matches_with_mediated.csv')\n",
    "tables_to_columns = dict()\n",
    "# Per tutte le tabelle in match nella colonna table 2 vado a prendermi\n",
    "# le colonne con match >= 0.5 con una colonna dello schema mediato\n",
    "for table_name in set(matches['table_2']):\n",
    "    # Prende le colonne con table 2 uguale a table name\n",
    "    table = matches.loc[matches['table_2'] == table_name]\n",
    "    columns = set()\n",
    "    for ind in table.index:\n",
    "        if table['match_value'][ind] >= 0.5:\n",
    "            columns.add((table['column_table_1'][ind], table['column_table_2'][ind]))\n",
    "    tables_to_columns[table_name] = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot concat schema final with:  FR-ft.json\n",
      "Cannot concat schema final with:  DDD-ft.com.json\n",
      "Cannot concat schema final with:  Silvestri-ft.com.json\n",
      "Cannot concat schema final with:  Gren-ft.com.json\n"
     ]
    }
   ],
   "source": [
    "schema_final = pd.DataFrame(columns=mediated_schema_columns)\n",
    "\n",
    "# Per tutti i dataset vai a inserire i dati nello schema mediato\n",
    "for df_name in tables_to_columns.keys():\n",
    "    df_aligned = pd.DataFrame()\n",
    "    \n",
    "    # Per tutti i match del dataset effettua una proiezione sulle colonne in match e rinominale per allinearle allo schema mediato\n",
    "    for match in tables_to_columns[df_name]:    \n",
    "        tmp = data_frames[df_name][match[1]].to_frame()\n",
    "        tmp = tmp.rename(columns={match[1]: match[0]})\n",
    "        df_aligned = pd.concat([df_aligned, tmp], axis=1)\n",
    "\n",
    "    try:\n",
    "        schema_final = pd.concat([schema_final, df_aligned], ignore_index=True, sort=False)\n",
    "    except:\n",
    "        print(\"Cannot concat schema final with: \", df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            name country market cap  \\\n",
      "0                             Berkshire Hathaway     NaN        NaN   \n",
      "1                                           ICBC     NaN        NaN   \n",
      "2       Saudi Arabian Oil Company (Saudi Aramco)     NaN        NaN   \n",
      "3                                 JPMorgan Chase     NaN        NaN   \n",
      "4                        China Construction Bank     NaN        NaN   \n",
      "...                                          ...     ...        ...   \n",
      "181405                             Daily Journal     NaN    $0.40 B   \n",
      "181406                                    Ouster     NaN    $0.22 B   \n",
      "181407                                    Ouster     NaN    $0.22 B   \n",
      "181408                                    Ouster     NaN    $0.22 B   \n",
      "181409                      Poseida Therapeutics     NaN    $0.60 B   \n",
      "\n",
      "       founded year employees                        industry sector  ceo  \\\n",
      "0            1939.0       NaN          Diversified Financials    NaN  NaN   \n",
      "1            1984.0       NaN                         Banking    NaN  NaN   \n",
      "2            1933.0       NaN            Oil & Gas Operations    NaN  NaN   \n",
      "3            2000.0       NaN  Banking and Financial Services    NaN  NaN   \n",
      "4            2014.0       NaN                         Banking    NaN  NaN   \n",
      "...             ...       ...                             ...    ...  ...   \n",
      "181405          NaN     290.0                             NaN    NaN  NaN   \n",
      "181406          NaN     290.0                             NaN    NaN  NaN   \n",
      "181407          NaN     290.0                             NaN    NaN  NaN   \n",
      "181408          NaN     290.0                             NaN    NaN  NaN   \n",
      "181409          NaN     290.0                             NaN    NaN  NaN   \n",
      "\n",
      "         revenue Stock share price city address website  \n",
      "0        $276.1B   NaN         NaN  NaN     NaN     NaN  \n",
      "1        $208.1B   NaN         NaN  NaN     NaN     NaN  \n",
      "2        $400.4B   NaN         NaN  NaN     NaN     NaN  \n",
      "3        $124.5B   NaN         NaN  NaN     NaN     NaN  \n",
      "4        $202.1B   NaN         NaN  NaN     NaN     NaN  \n",
      "...          ...   ...         ...  ...     ...     ...  \n",
      "181405     $54 M   NaN     $294.40  NaN     NaN     NaN  \n",
      "181406  $41.94 M   NaN       $1.21  NaN     NaN     NaN  \n",
      "181407  $41.94 M   NaN       $1.21  NaN     NaN     NaN  \n",
      "181408  $41.94 M   NaN       $1.21  NaN     NaN     NaN  \n",
      "181409   $0.15 B   NaN       $7.04  NaN     NaN     NaN  \n",
      "\n",
      "[181410 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "schema_final = pd.read_csv(csv_path + \"schema_final.csv\", index_col=0)\n",
    "schema_final = schema_final.reset_index(drop=True)\n",
    "# schema_final.astype({\"id\": Int64})\n",
    "# idx = {\"id\": list(schema_final.index)}\n",
    "# schema_final = pd.concat([pd.DataFrame(idx), schema_final], axis=1)\n",
    "# schema_final['id'] = schema_final['id'].astype('Int64')\n",
    "print(schema_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_final.to_csv(csv_path + \"schema_final.csv\", index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = schema_final.iloc[:10]\n",
    "sch.to_csv(csv_path + \"schema_final_10.csv\", index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20fe05bf6d812f8581d0c3885f26a8156e46b0ff73914791c267a01ece645732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
