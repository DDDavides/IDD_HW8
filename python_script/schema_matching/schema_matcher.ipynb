{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valentine import valentine_match, valentine_metrics\n",
    "from valentine.algorithms import Coma\n",
    "from valentine.algorithms import Cupid\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pandas as pd\n",
    "from compute_match import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparazione data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './csv/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dizionari che associa il nome del file al file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset sono in formato json necessariamente\n",
    "path = \"../../json_datasets\"\n",
    "\n",
    "# Dizionario Key=Nome del file, Value=File json associato (letto da filepath)\n",
    "data_frames = {}\n",
    "# Per tutti i file dentro la cartella dei dataset\n",
    "for file in os.listdir(path):\n",
    "    # Prendo il path del file\n",
    "    filepath = f\"{path}/{file}\"\n",
    "    # Update del dizionario\n",
    "    data_frames[file] = (pd.read_json(filepath))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione di tutte le coppie di dataset da valutare (evitando ripetizioni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendo tutti i nomi dei file\n",
    "keys = list(data_frames.keys())\n",
    "tuples = []\n",
    "# Il numero di dataframes\n",
    "dfs_len = len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preso un dataframe, lo confronto con tutti i dataframes successivi (così evito di confrontare coppie già confrontate)\n",
    "for i in range(dfs_len):\n",
    "    for j in range(i + 1, dfs_len):\n",
    "        dfl = data_frames[keys[i]]\n",
    "        dfr = data_frames[keys[j]]\n",
    "        # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "        tuples.append((keys[i], keys[j], dfl, dfr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valentine schema matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_multithread(tuples):\n",
    "    result = []\n",
    "    # Creo un pool per l'esecuzione multi processore (per velocizzare il matching tra tutte le coppie di dataframes)\n",
    "    with Pool() as pool:\n",
    "        # Ogni pool esegue la funzione (primo parametro), passandogli le tuple (secondo parametro)\n",
    "        # result è una lista di tuple ove una tupla è il risultato della funzione mappata\n",
    "        result = pool.map(calculate_match_coma_schema, tuples)\n",
    "    return result\n",
    "\n",
    "def matches_to_table(result):\n",
    "    matches = pd.DataFrame()\n",
    "    # Per ogni risultato ottenuto dal matching\n",
    "    for match in result:\n",
    "    # Prendo il valore ritornato dalla funzione valentine_match \n",
    "    # e.g. match[2] = ((table_1, 'Cited by'),(table_2, 'Cited by')): 0.83\n",
    "        for key in match[2].keys():\n",
    "            # e.g. key = (table_1, 'Cited by'),(table_2, 'Cited by')\n",
    "            d = dict()\n",
    "            d[\"table_1\"] = match[0]\n",
    "            d[\"table_2\"] = match[1]\n",
    "            d[\"column_table_1\"] = key[0][1]\n",
    "            d[\"column_table_2\"] = key[1][1]\n",
    "            d[\"match_value\"] = match[2][key]\n",
    "            # Aggiungiamo al dataframe l'ennupla\n",
    "            matches = pd.concat([matches, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    return matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabella per la visualizzazione dei risultati dello schema matching con Valentine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "result = calculate_match_multithread(tuples)\n",
    "finish_time = time.perf_counter()\n",
    "\n",
    "# Definizione del nome delle colonne della tabella\n",
    "columns = [\"table_1\", \"table_2\", \"column_table_1\", \"column_table_2\", \"match_value\"]\n",
    "# Creo il dataframe (la tabella in questione)\n",
    "matches = pd.DataFrame(columns=columns)\n",
    "\n",
    "matches = matches_to_table(result)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matches).to_csv(csv_path + \"matches.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema mediato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediated_schema_columns = ('name', 'country', 'market cap', 'founded year', 'employees', 'industry', 'sector',\n",
    "     'ceo', 'revenue', 'Stock', 'share price', 'city', 'address', 'website')\n",
    "\n",
    "mediated_schema = pd.DataFrame(columns=mediated_schema_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_len = len(keys)\n",
    "matches_with_mediated = {}\n",
    "for i in range(dfs_len):\n",
    "    dfl = mediated_schema\n",
    "    dfr = data_frames[keys[i]]\n",
    "    # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "    tuples.append(('mediated_schema', keys[i], dfl, dfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = calculate_match_multithread(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches_to_table(result)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matches).to_csv(csv_path + \"matches_with_mediated.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ce6619c7d95fa66adab3aa560eb20f28a01e1f23fb927f9fbc92f5fa3f739e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
