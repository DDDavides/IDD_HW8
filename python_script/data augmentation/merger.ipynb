{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_to_join_with_path = \"./csv/tables_to_join/\"\n",
    "# tables_to_join_with_path = \"./csv/to_test/\" # for testing\n",
    "schema_augmented_path = \"./csv/schema_augmentated.csv\"\n",
    "# schema_augmented_path = \"./csv/test_schema_augmentated.csv\" # for testing\n",
    "\n",
    "schema_linked_to_join_path = \"../record_linkage/csv/schema_linked.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c'è la tabella 21a che ha righe duplicate quindi non la usiamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_idTable_to_join_with = [\"Security_5a7098f0c868a712fd740a7b\", \"Company_5a708e29c868a712fd72e48d\", \"Airline_5a70aa27c868a712fd75b3a4\", \"Company_5a709ca3c868a712fd746a77\", \"Name_5a708f2dc868a712fd72f7f0\", \"Name_5a7098e5c868a712fd740889\", \"Make_5a708ca3c868a712fd72c735\", \"Company_5a70ab31c868a712fd75cc0e\", \"Name_5a70928ec868a712fd7343db\", \"Airline_5a708f2fc868a712fd72f81c\", \"Company_5a70a301c868a712fd7511ce\", \"English_5a708b9fc868a712fd72ba3b\", \"Name_5a708d9fc868a712fd72d9e4\", \"Name_5a70a031c868a712fd74c51c\", \"Company_5a708b7ac868a712fd72b7c6\", \"Company_5a7098d3c868a712fd7406d5\", \"Manufacturer_5a70d572c868a712fd79767a\", \"English_5a708b9fc868a712fd72ba3c\", \"Company_5a70a2bac868a712fd75071a\", \"Name_5a708e81c868a712fd72ec57\", \"Company_5a709467c868a712fd737622\", \"Company_5a70943dc868a712fd73721a\", \"English_5a708b9fc868a712fd72ba36\", \"Manufacturer_5a709ec0c868a712fd749c60\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/dnpzswqd7ll8qj2knwvlk3v80000gn/T/ipykernel_4285/1084007032.py:1: DtypeWarning: Columns (5,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  schema_linked_to_join = pd.read_csv(schema_linked_to_join_path)\n"
     ]
    }
   ],
   "source": [
    "schema_linked_to_join = pd.read_csv(schema_linked_to_join_path)\n",
    "schema_linked_to_join = schema_linked_to_join.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valentine import valentine_match, valentine_metrics\n",
    "from valentine.algorithms import Coma\n",
    "from valentine.algorithms import Cupid\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pandas as pd\n",
    "from compute_match import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione con cui si calcola il matching parallelizzando su più processori\n",
    "def calculate_match_multithread(tuples):\n",
    "    result = [] # result è una lista di tuple (ove una tupla è il risultato della funzione mappata)\n",
    "    # Creo un pool per l'esecuzione multi processore (per velocizzare il matching tra tutte le coppie di dataframes)\n",
    "    with Pool() as pool:\n",
    "        # Ogni pool esegue la funzione (primo parametro), passandogli le tuple (secondo parametro)\n",
    "        result = pool.map(calculate_match_coma_schema, tuples)\n",
    "    return result\n",
    "\n",
    "# funzione che riporta un dataframe contenente le informazioni circa tutti i match passati come parametro\n",
    "# @param result, corrisponde al risultato del matching (i.e. nome dataset1, nome dataset2, il valore del matching di valentine)\n",
    "def matches_to_table(result):\n",
    "    matches = pd.DataFrame()\n",
    "    # Per ogni risultato ottenuto dal matching parallelizzato\n",
    "    for match in result:\n",
    "    # Prendo il valore di matching ritornato dalla funzione valentine_match \n",
    "    # e.g. match[2] = ((table_1, 'Cited by'),(table_2, 'Cited by')): 0.83\n",
    "        for key in match[2].keys():\n",
    "            # e.g. key = (table_1, 'Cited by'),(table_2, 'Cited by')\n",
    "            d = dict()\n",
    "            d[\"table_1\"] = match[0]\n",
    "            d[\"table_2\"] = match[1]\n",
    "            d[\"column_table_1\"] = key[0][1]\n",
    "            d[\"column_table_2\"] = key[1][1]\n",
    "            d[\"match_value\"] = match[2][key]\n",
    "            # Aggiungiamo al dataframe l'ennupla\n",
    "            matches = pd.concat([matches, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = {}\n",
    "for file in os.listdir(tables_to_join_with_path):\n",
    "    table_id = file.split(\"_\")[-1].split(\".\")[0]\n",
    "    data_frames[table_id] = pd.read_csv(tables_to_join_with_path + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione di tutte le coppie (schema-mediato, dataset)\n",
    "matches_with_mediated = {}\n",
    "all_mediatedNDbs_couples = []\n",
    "for dbId in data_frames:\n",
    "    dfl = schema_linked_to_join\n",
    "    dfr = data_frames[dbId]\n",
    "    # le tuple da passare al metodo di matching = nome dataset1, nome dataset2, dataframe1, dataframe2\n",
    "    all_mediatedNDbs_couples.append(('mediated_schema', dbId, dfl, dfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches con lo schema mediato\n",
    "result = calculate_match_multithread(all_mediatedNDbs_couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches_to_table(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(matches).to_csv(\"./csv/matches_with_mediated_N_wikitables.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsTableMatches = pd.read_csv(\"./csv/matches_with_mediated_N_wikitables.csv\")\n",
    "tables_to_columns = dict()\n",
    "# Per tutte le tabelle in match nella colonna table 2 vado a prendermi\n",
    "# le colonne con match >= 0.5 con una colonna dello schema mediato\n",
    "for table_name in set(colsTableMatches['table_2']):\n",
    "    # Prende le colonne con table 2 uguale a table name\n",
    "    table = colsTableMatches.loc[colsTableMatches['table_2'] == table_name]\n",
    "    columns = set()\n",
    "    for ind in table.index:\n",
    "        if table['match_value'][ind] >= 0.4:\n",
    "            columns.add((table['column_table_1'][ind], table['column_table_2'][ind]))\n",
    "    tables_to_columns[table_name] = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = [\"nan\", \"Nan\", \"NaN\", \"Not found\", \"Not Found\", \"not found\", \"not Found\", \"NULL\", \"null\", \"Null\", \"None\", \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(tables_to_join_with_path):\n",
    "    # leggo il db corrente con cui fare un eventuale join\n",
    "    curr_db_to_join_with = pd.read_csv(tables_to_join_with_path + file)\n",
    "\n",
    "    # metto le colonne della tabella da joinare in lower case\n",
    "    curr_db_to_join_with.columns = map(str.lower, curr_db_to_join_with.columns)\n",
    "    \n",
    "    # id table della tabella appena letta\n",
    "    table_id = file.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "    ## prendo i match della tabella corrente con lo schema mediato\n",
    "    table_to_columns_matches = tables_to_columns[table_id]\n",
    "\n",
    "    joined = False\n",
    "    # controllo che questa tabella sia tra le tabelle da joinare\n",
    "    for col_dbId in col_idTable_to_join_with:\n",
    "        col_N_idTable = col_dbId.split(\"_\") # colonna e id\n",
    "        curr_id_table = col_N_idTable[1] # id\n",
    "\n",
    "        # è una delle tabelle con cui devo joinare\n",
    "        if table_id == curr_id_table:\n",
    "\n",
    "            column_to_join = col_N_idTable[0].lower() # colonna cu cui fare il join\n",
    "\n",
    "            to_join_after = curr_db_to_join_with # il join lo faccio prima con un dataset a cui tolgo le colonne già presenti nello schema\n",
    "            column_to_join_after = [] # colonne droppate perché già presenti nello schema, i cui dati verranno aggiunti dopo\n",
    "            \n",
    "            # per ogni colonna nel dataset da joinare\n",
    "            for colDB in curr_db_to_join_with.columns:\n",
    "                # se la colonna non è quella su cui fare il join ed è già presente nello schema\n",
    "                if((colDB != column_to_join) and (colDB in schema_linked_to_join.columns)):\n",
    "\n",
    "                    # droppo la colonna che poi aggiungerò in seguito\n",
    "                    column_to_join_after.append(colDB)\n",
    "                    to_join_after = to_join_after.drop(colDB, axis=1)\n",
    "                \n",
    "                ## altrimenti se la colonna non è nello schema e non è quella di join\n",
    "                elif((colDB != column_to_join) and (colDB not in schema_linked_to_join.columns)):\n",
    "                    ## se la colonna è in match con una colonna dello schema, allora la aggiungo dopo\n",
    "                    for (colSchema, colTable) in table_to_columns_matches:\n",
    "                        if(colDB==colTable.lower()):\n",
    "                            ## aggiunta nelle colonne da joinare dopo\n",
    "                            column_to_join_after.append(colDB)\n",
    "                            ## e poi droppata\n",
    "                            to_join_after = to_join_after.drop(colDB, axis=1)\n",
    "\n",
    "            # modifico il nome della colonna su cui fare join => \"name\"\n",
    "            to_join_after = to_join_after.rename(columns={column_to_join: \"name\"})\n",
    "\n",
    "            # eseguo il merge sul campo \"name\"\n",
    "            schema_linked_to_join = schema_linked_to_join.merge(to_join_after, on=\"name\", how=\"outer\", suffixes=[\"\",\"\"])\n",
    "\n",
    "            # se ci sono colonne droppate allora le inserisco\n",
    "            if(len(column_to_join_after) != 0):\n",
    "                # riempio le colonne droppate di tutte le righe corrispondenti a quelle presenti nel dataset da joinare\n",
    "                for nameCompany in curr_db_to_join_with[column_to_join]:\n",
    "                    # id righe\n",
    "                    idRowToJoin = curr_db_to_join_with.loc[curr_db_to_join_with[column_to_join]==nameCompany].index[0] # id della riga dell'azienda da joinare nel dataset da joinare\n",
    "                    idRow = schema_linked_to_join.loc[schema_linked_to_join[\"name\"]==nameCompany].index[0] # id della riga della stessa azienda ma nello schema\n",
    "\n",
    "                    # riempio tutte le colonne droppate\n",
    "                    for col in column_to_join_after:\n",
    "                        ## se la colonna in questione è in match con una colonna dello schema\n",
    "                        ## metto il contenuto dentro la colonna con cui sta in match\n",
    "                        for (colSchema, colTable) in table_to_columns_matches:\n",
    "                            if(col==colTable.lower()):\n",
    "                                curr_data = schema_linked_to_join.iloc[idRow][colSchema] # dato presente nella cella corrente dello schema\n",
    "                                # se il dato è vuoto allora lo aggiorno altrimenti mantengo quello già presente (non faccio nulla)\n",
    "                                if(pd.isnull(curr_data) or (curr_data in nan)):\n",
    "                                    schema_linked_to_join.loc[schema_linked_to_join[\"name\"]==nameCompany, colSchema] = curr_db_to_join_with.iloc[idRowToJoin][col]\n",
    "                                    already_joined = True\n",
    "\n",
    "                        ## altrimenti aggiungo la colonna normalmente\n",
    "                        # per tutte le colonne prima droppate e ora da aggiungere\n",
    "                        # setto il dato nella cella dentro lo schema linked\n",
    "                        curr_data = schema_linked_to_join.iloc[idRow][colSchema] # dato nella cella corrente\n",
    "                        # se il dato non è stato inserito e la cella non è vuota allora aggiungo il dato, altrimenti no\n",
    "                        if(not already_joined and (pd.isnull(curr_data) or (curr_data in nan))):\n",
    "                            schema_linked_to_join.loc[schema_linked_to_join[\"name\"]==nameCompany, col] = curr_db_to_join_with.iloc[idRowToJoin][col]\n",
    "            \n",
    "            joined = True # join completato\n",
    "\n",
    "    if(not joined):\n",
    "        print(file, \"not joined\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# con stampe per testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in os.listdir(tables_to_join_with_path):\n",
    "#     # leggo il db corrente con cui fare un eventuale join\n",
    "#     curr_db_to_join_with = pd.read_csv(tables_to_join_with_path + file)\n",
    "\n",
    "#     # metto le colonne della tabella da joinare in lower case\n",
    "#     curr_db_to_join_with.columns = map(str.lower, curr_db_to_join_with.columns)\n",
    "    \n",
    "#     # id table della tabella appena letta\n",
    "#     table_id = file.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "#     ## prendo i match della tabella corrente con lo schema mediato\n",
    "#     table_to_columns_matches = tables_to_columns[table_id]\n",
    "#     print(\"Matches:\", table_to_columns_matches)\n",
    "\n",
    "#     joined = False\n",
    "#     # controllo che questa tabella sia tra le tabelle da joinare\n",
    "#     for col_dbId in col_idTable_to_join_with:\n",
    "#         col_N_idTable = col_dbId.split(\"_\") # colonna e id\n",
    "#         curr_id_table = col_N_idTable[1] # id\n",
    "\n",
    "#         # è una delle tabelle con cui devo joinare\n",
    "#         if table_id == curr_id_table:\n",
    "#             print(\"Current db:\", file)\n",
    "#             display(\"To join with (lower):\", curr_db_to_join_with)\n",
    "\n",
    "#             column_to_join = col_N_idTable[0].lower() # colonna cu cui fare il join\n",
    "#             print(\"Column to join:\", column_to_join)\n",
    "\n",
    "#             to_join_after = curr_db_to_join_with # il join lo faccio prima con un dataset a cui tolgo le colonne già presenti nello schema\n",
    "#             column_to_join_after = [] # colonne droppate perché già presenti nello schema, i cui dati verranno aggiunti dopo\n",
    "            \n",
    "#             # per ogni colonna nel dataset da joinare\n",
    "#             for colDB in curr_db_to_join_with.columns:\n",
    "#                 # se la colonna non è quella su cui fare il join ed è già presente nello schema\n",
    "#                 if((colDB != column_to_join) and (colDB in schema_linked_to_join.columns)):\n",
    "#                     print(\"Da aggiungere dopo (già presente):\", colDB)\n",
    "#                     # droppo la colonna che poi aggiungerò in seguito\n",
    "#                     column_to_join_after.append(colDB)\n",
    "#                     to_join_after = to_join_after.drop(colDB, axis=1)\n",
    "                \n",
    "#                 ## altrimenti se la colonna non è nello schema e non è quella di join\n",
    "#                 elif((colDB != column_to_join) and (colDB not in schema_linked_to_join.columns)):\n",
    "#                     ## se la colonna è in match con una colonna dello schema, allora la aggiungo dopo\n",
    "#                     for (colSchema, colTable) in table_to_columns_matches:\n",
    "#                         if(colDB==colTable.lower()):\n",
    "#                             print(\"Da aggiugnere dopo:\", colDB, \"matcha con\", colSchema)\n",
    "#                             ## aggiunta nelle colonne da joinare dopo\n",
    "#                             column_to_join_after.append(colDB)\n",
    "#                             ## e poi droppata\n",
    "#                             to_join_after = to_join_after.drop(colDB, axis=1)\n",
    "\n",
    "#             # modifico il nome della colonna su cui fare join => \"name\"\n",
    "#             to_join_after = to_join_after.rename(columns={column_to_join: \"name\"})\n",
    "#             display(\"After renaming in \\\"name\\\"\", to_join_after)\n",
    "\n",
    "#             # eseguo il merge sul campo \"name\"\n",
    "#             schema_linked_to_join = schema_linked_to_join.merge(to_join_after, on=\"name\", how=\"outer\", suffixes=[\"\",\"\"])\n",
    "#             display(\"Prima del joining con drop\", schema_linked_to_join)\n",
    "\n",
    "#             # se ci sono colonne droppate allora le inserisco\n",
    "#             if(len(column_to_join_after) != 0):\n",
    "#                 print(\"Devo aggiungere le colonne droppate\")\n",
    "#                 # riempio le colonne droppate di tutte le righe corrispondenti a quelle presenti nel dataset da joinare\n",
    "#                 for nameCompany in curr_db_to_join_with[column_to_join]:\n",
    "#                     print(\"Devo aggiungere all'azienda\", nameCompany)\n",
    "#                     # id righe\n",
    "#                     idRowToJoin = curr_db_to_join_with.loc[curr_db_to_join_with[column_to_join]==nameCompany].index[0] # id della riga dell'azienda da joinare nel dataset da joinare\n",
    "#                     idRow = schema_linked_to_join.loc[schema_linked_to_join[\"name\"]==nameCompany].index[0] # id della riga della stessa azienda ma nello schema\n",
    "\n",
    "#                     # riempio tutte le colonne droppate\n",
    "#                     for col in column_to_join_after:\n",
    "#                         ## se la colonna in questione è in match con una colonna dello schema\n",
    "#                         ## metto il contenuto dentro la colonna con cui sta in match\n",
    "#                         for (colSchema, colTable) in table_to_columns_matches:\n",
    "#                             if(col==colTable.lower()):\n",
    "#                                 curr_data = schema_linked_to_join.iloc[idRow][colSchema]\n",
    "#                                 print(\"Aggiungo colonne in match\", colTable, \"con\", colSchema)\n",
    "#                                 # se il dato è vuoto allora lo aggiorno altrimenti mantengo quello già presente\n",
    "#                                 if(pd.isnull(curr_data) or (curr_data in nan)):\n",
    "#                                     schema_linked_to_join.loc[schema_linked_to_join[\"name\"]==nameCompany, colSchema] = curr_db_to_join_with.iloc[idRowToJoin][col]\n",
    "#                                     already_joined = True\n",
    "#                                     print(\"Dopo aggiunta del matching alla colonna\", colSchema, \"del valore\", curr_db_to_join_with.iloc[idRowToJoin][col])\n",
    "#                                     display(schema_linked_to_join.iloc[idRow])\n",
    "#                                 else:\n",
    "#                                     print(\"Il dato già c'è nella colonna\", colSchema, \":\", curr_data)\n",
    "\n",
    "#                         ## altrimenti aggiungo la colonna normalmente\n",
    "#                         # per tutte le colonne prima droppate e ora da aggiungere\n",
    "#                         # setto il dato nella cella dentro lo schema linked\n",
    "#                         curr_data = schema_linked_to_join.iloc[idRow][colSchema] # dato nella cella corrente\n",
    "#                         # se il dato non è stato inserito e la cella non è vuota allora aggiungo il dato\n",
    "#                         if(not already_joined and (pd.isnull(curr_data) or (curr_data in nan))):\n",
    "#                             schema_linked_to_join.loc[schema_linked_to_join[\"name\"]==nameCompany, col] = curr_db_to_join_with.iloc[idRowToJoin][col]\n",
    "#                             display(\"Dopo aggiunta colonna già presente\", schema_linked_to_join.iloc[idRow])\n",
    "#                         elif(not already_joined): \n",
    "#                             print(\"Il dato già c'è nella colonna\", colSchema, \":\", curr_data)\n",
    "            \n",
    "#             joined = True # join completato\n",
    "\n",
    "#     if(not joined):\n",
    "#         print(file, \"not joined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_linked_to_join.to_csv(schema_augmented_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>market cap</th>\n",
       "      <th>founded year</th>\n",
       "      <th>employees</th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>ceo</th>\n",
       "      <th>revenue</th>\n",
       "      <th>stock</th>\n",
       "      <th>...</th>\n",
       "      <th>russian</th>\n",
       "      <th>notes</th>\n",
       "      <th>profits (billion $)</th>\n",
       "      <th>assets (billion $)</th>\n",
       "      <th>market value (billion $)</th>\n",
       "      <th>countries listed</th>\n",
       "      <th>products / solutions</th>\n",
       "      <th>tse</th>\n",
       "      <th>rōmaji</th>\n",
       "      <th>japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>superit srl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finance insurance and real estate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>send to me srl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intesa sanpaolo vita spa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finance insurance and real estate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>totalenergies petrochemicals &amp; refining sa nv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wholesale trade</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanastera spa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finance insurance and real estate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74187</th>\n",
       "      <td>Yageo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74188</th>\n",
       "      <td>Yang Ming Marine Transport Corporation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74189</th>\n",
       "      <td>Yeh-Chiang Technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74190</th>\n",
       "      <td>Yulon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74191</th>\n",
       "      <td>ZyXEL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74192 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name country market cap  \\\n",
       "0                                        superit srl     NaN        NaN   \n",
       "1                                     send to me srl     NaN        NaN   \n",
       "2                           intesa sanpaolo vita spa     NaN        NaN   \n",
       "3      totalenergies petrochemicals & refining sa nv     NaN        NaN   \n",
       "4                                      sanastera spa     NaN        NaN   \n",
       "...                                              ...     ...        ...   \n",
       "74187                                          Yageo     NaN        NaN   \n",
       "74188         Yang Ming Marine Transport Corporation     NaN        NaN   \n",
       "74189                          Yeh-Chiang Technology     NaN        NaN   \n",
       "74190                                          Yulon     NaN        NaN   \n",
       "74191                                          ZyXEL     NaN        NaN   \n",
       "\n",
       "      founded year employees                           industry sector  ceo  \\\n",
       "0              NaN       NaN  finance insurance and real estate    NaN  NaN   \n",
       "1              NaN       NaN                           services    NaN  NaN   \n",
       "2              NaN       NaN  finance insurance and real estate    NaN  NaN   \n",
       "3              NaN       NaN                    wholesale trade    NaN  NaN   \n",
       "4              NaN       NaN  finance insurance and real estate    NaN  NaN   \n",
       "...            ...       ...                                ...    ...  ...   \n",
       "74187          NaN       NaN                                NaN    NaN  NaN   \n",
       "74188          NaN       NaN                                NaN    NaN  NaN   \n",
       "74189          NaN       NaN                                NaN    NaN  NaN   \n",
       "74190          NaN       NaN                                NaN    NaN  NaN   \n",
       "74191          NaN       NaN                                NaN    NaN  NaN   \n",
       "\n",
       "      revenue stock  ... russian notes profits (billion $) assets (billion $)  \\\n",
       "0         NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "1         NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "2         NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "3         NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "4         NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "...       ...   ...  ...     ...   ...                 ...                ...   \n",
       "74187     NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "74188     NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "74189     NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "74190     NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "74191     NaN   NaN  ...     NaN   NaN                 NaN                NaN   \n",
       "\n",
       "      market value (billion $) countries listed products / solutions  tse  \\\n",
       "0                          NaN              NaN                  NaN  NaN   \n",
       "1                          NaN              NaN                  NaN  NaN   \n",
       "2                          NaN              NaN                  NaN  NaN   \n",
       "3                          NaN              NaN                  NaN  NaN   \n",
       "4                          NaN              NaN                  NaN  NaN   \n",
       "...                        ...              ...                  ...  ...   \n",
       "74187                      NaN              NaN                  NaN  NaN   \n",
       "74188                      NaN              NaN                  NaN  NaN   \n",
       "74189                      NaN              NaN                  NaN  NaN   \n",
       "74190                      NaN              NaN                  NaN  NaN   \n",
       "74191                      NaN              NaN                  NaN  NaN   \n",
       "\n",
       "      rōmaji japanese  \n",
       "0        NaN      NaN  \n",
       "1        NaN      NaN  \n",
       "2        NaN      NaN  \n",
       "3        NaN      NaN  \n",
       "4        NaN      NaN  \n",
       "...      ...      ...  \n",
       "74187    NaN      NaN  \n",
       "74188    NaN      NaN  \n",
       "74189    NaN      NaN  \n",
       "74190    NaN      NaN  \n",
       "74191    NaN      NaN  \n",
       "\n",
       "[74192 rows x 44 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_linked_to_join"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20fe05bf6d812f8581d0c3885f26a8156e46b0ff73914791c267a01ece645732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
